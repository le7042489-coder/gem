{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def get_all_json_files(directory):\n",
    "    directory_path = Path(directory)\n",
    "    return [str(file) for file in directory_path.rglob('*.json')]\n",
    "\n",
    "def load_json_files(json_file_paths):\n",
    "    json_data_list = []\n",
    "    error_file_list = []\n",
    "    for file_path in json_file_paths:\n",
    "        try:\n",
    "            with open(file_path, 'r') as json_file:\n",
    "                data = json.load(json_file)  # Load JSON content\n",
    "                json_data_list.append(data)  # Add to list\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file_path}: {e}\")\n",
    "            error_file_list.append(file_path)\n",
    "    return json_data_list, error_file_list\n",
    "\n",
    "# Example usage\n",
    "model_name = \"gem7b\"\n",
    "\n",
    "directory_path = f\"GEM_Evaluation_PTBXL/gpt_evaluated/{model_name}\"  # Replace with your actual path\n",
    "all_json_files = get_all_json_files(directory_path)\n",
    "print(len(all_json_files))\n",
    "json_data, error_data = load_json_files(all_json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}\n",
    "for d in json_data:\n",
    "    all_results[d['id']] = d['results']\n",
    "\n",
    "save_path = f\"GEM_Evaluation_PTBXL/gpt_evaluated_all/{model_name}_all_results.json\"\n",
    "\n",
    "os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(all_results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"gem7b\"\n",
    "\n",
    "directory_path = f\"GEM_Evaluation_PTBXL/gpt_evaluated_all/{model_name}_all_results.json\"\n",
    "\n",
    "with open(directory_path, 'r') as json_file:\n",
    "    json_data = json.load(json_file)  # Load JSON content\n",
    "\n",
    "expected_keys = [\n",
    "    'DiagnosisAccuracy',\n",
    "    'AnalysisCompleteness',\n",
    "    'AnalysisRelevance',\n",
    "    'LeadAssessmentCoverage',\n",
    "    'LeadAssessmentAccuracy',\n",
    "    'ECGFeatureGrounding',\n",
    "    'EvidenceBasedReasoning',\n",
    "    'ClinicalDiagnosticFidelity'\n",
    "]\n",
    "\n",
    "pattern = re.compile(r'\\\"(?P<key>{})\\\":\\s*(?P<content>\\[.*?\\])'.format('|'.join(expected_keys)), re.DOTALL)\n",
    "\n",
    "result = {}\n",
    "\n",
    "# Additional cleaning functions\n",
    "def fix_unterminated_string(content):\n",
    "    quote_count = len(re.findall(r'(?<!\\\\)\"', content))\n",
    "    if quote_count % 2 == 1:\n",
    "        content = re.sub(r'(\\s*[}\\]])', r'\"\\1', content, count=1)\n",
    "    return content\n",
    "\n",
    "def escape_inner_quotes_in_explanation(content):\n",
    "    def replacer(match):\n",
    "        explanation = match.group(1)\n",
    "        fixed = re.sub(r'(?<!\\\\)\"', r'\\\\\"', explanation)\n",
    "        return f'\"Explanation\": \"{fixed}\"'\n",
    "    return re.sub(r'\"Explanation\":\\s*\"([^\"]*?)\"', replacer, content)\n",
    "\n",
    "def remove_extra_quotes(content):\n",
    "    content = re.sub(r'\"\"+', '\"', content)\n",
    "    return content\n",
    "\n",
    "def fix_unmatched_brackets(content):\n",
    "    def replacer(match):\n",
    "        explanation = match.group(1)\n",
    "        fixed = re.sub(r'[\\[\\]]', '', explanation)\n",
    "        return f'\"Explanation\": \"{fixed}\"'\n",
    "    return re.sub(r'\"Explanation\":\\s*\"([^\"]*?)\"', replacer, content)\n",
    "\n",
    "\n",
    "def fix_missing_commas(content):\n",
    "    content = re.sub(r'(\\})(\\s*\\{)', r'\\1,\\2', content)\n",
    "    return content\n",
    "\n",
    "def safe_eval(match):\n",
    "    try:\n",
    "        return str(eval(match.group(1)))\n",
    "    except:\n",
    "        return match.group(1)  # Return the original string if eval fails\n",
    "\n",
    "for id, content in json_data.items():\n",
    "\n",
    "    json_content = content.strip('```json\\n').strip('\\n```')\n",
    "    result[id] = {}\n",
    "\n",
    "    matches = pattern.finditer(json_content)\n",
    "    \n",
    "    for match in matches:\n",
    "        key = match.group('key')\n",
    "        content = match.group('content')\n",
    "\n",
    "        # Original cleaning steps\n",
    "        content = content.replace(\"\\\"\", '\"').replace(\"“\", '\"').replace(\"”\", '\"')\n",
    "        content = re.sub(r'//.*', '', content)\n",
    "        content = re.sub(r',\\s*([}\\]])', r'\\1', content)\n",
    "        content = re.sub(r'\"\\s*\"', ' ', content)\n",
    "        content = re.sub(r'\\+(\\d)', r'\\1', content)\n",
    "        content = re.sub(r'(\\d+[\\d\\s\\*\\+\\-\\/]+\\d+)', safe_eval, content)\n",
    "        content = content.replace('\");', '\"')\n",
    "        content = re.sub(r'\\s+', ' ', content)\n",
    "        content = re.sub(r'\\n|\\r', ' ', content)\n",
    "\n",
    "        # Remove unmatched trailing characters after quote\n",
    "        content = re.sub(r'\"\\s*[\\]\\)]', '\"', content)\n",
    "        # Fix missing commas between JSON objects in arrays\n",
    "        content = re.sub(r'\\}\\s*\\{', '},{', content)\n",
    "        content = re.sub(r'(\"Explanation\":\\s*\".*?)(?<!\\\\)\"\\s*,?\\s*\\{', r'\\1\"}, {', content)\n",
    "\n",
    "        # Additional cleaning steps\n",
    "        content = fix_unterminated_string(content)\n",
    "        content = escape_inner_quotes_in_explanation(content)\n",
    "        content = remove_extra_quotes(content)\n",
    "        content = fix_unmatched_brackets(content)\n",
    "        content = fix_missing_commas(content)\n",
    "\n",
    "        open_braces = content.count('{')\n",
    "        close_braces = content.count('}')\n",
    "        if open_braces > close_braces:\n",
    "            content += '}' * (open_braces - close_braces)\n",
    "        \n",
    "        open_brackets = content.count('[')\n",
    "        close_brackets = content.count(']')\n",
    "        if open_brackets > close_brackets:\n",
    "            content += ']' * (open_brackets - close_brackets)\n",
    "\n",
    "        try:\n",
    "            content_json = json.loads(content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decoding error for id {id}, key {key}: {e}\")\n",
    "            print(\"Content:\", content)\n",
    "            continue\n",
    "\n",
    "        scores = []\n",
    "        explanations = []\n",
    "\n",
    "        for item in content_json:\n",
    "            score = item.get('Score')\n",
    "            explanation = item.get('Explanation', '').strip()\n",
    "            extra_fields = {k: v for k, v in item.items() if k not in ['Score', 'Explanation']}\n",
    "\n",
    "            if extra_fields:\n",
    "                explanation += \" Additional details: \" + json.dumps(extra_fields)\n",
    "\n",
    "            scores.append(score)\n",
    "            explanations.append(explanation)\n",
    "\n",
    "        result[id][key] = {\n",
    "            'Scores': scores,\n",
    "            'Explanations': explanations\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for id, content in result.items():\n",
    "    results[id] = {}\n",
    "    for key, value in content.items():\n",
    "        \n",
    "        if key in ['LeadAssessmentCoverage', 'LeadAssessmentAccuracy', 'AnalysisCompleteness', 'AnalysisRelevance', 'ECGFeatureGrounding', 'EvidenceBasedReasoning', 'ClinicalDiagnosticFidelity']:\n",
    "            average = sum(value['Scores'])\n",
    "        else:\n",
    "            average = len([x for x in value['Scores'] if x > 0] ) / (len(value['Scores'])) if value['Scores'] else 0\n",
    "\n",
    "        results[id][key] = average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DiagnosisAccuracy'] = df['DiagnosisAccuracy'] * 100\n",
    "df['LeadAssessmentCoverage'] = df['LeadAssessmentCoverage'].clip(upper=12)/12 * 100\n",
    "df['LeadAssessmentAccuracy'] = df['LeadAssessmentAccuracy']/24 * 100\n",
    "\n",
    "df.mean().round(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llava",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
